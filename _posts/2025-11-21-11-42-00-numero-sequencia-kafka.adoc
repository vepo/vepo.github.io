---
title: Como implementar um seletor de números sequenciais no Apache Kafka
published: true
description: |
    Quando trabalhamos com sistemas distribuídos, muitas vezes precisamos de uma maneira de gerar números sequenciais únicos para identificar objetos. Existem diversas formas de implementar isso, mas as vezes algumas limitações técnicas nos obrigam a usar sistemas como o Apache Kafka para essa tarefa.
tags: [Ciência da Computação, Sistemas Distribuídos, Microsserviços]
cover_image: /assets/images/microservices/cover-100-42.webp
// series: Sistemas Distribuídos & Microsserviços
permalink: /posts/como-implementar-um-seletor-de-numeros-sequenciais-no-apache-kafka
publish_date: 2025-11-21 11:47:00 +0300
---

:figure-caption: Figura
:imagesdir: /assets/images/

Este relato é baseado em uma situação real do projeto em que atuo. Nossa solução utiliza Apache Kafka para processar CDRs (Call Detail Records), lidando com bilhões de registros diários. Todo o fluxo é construído em torno do Kafka, em um sistema que já opera em vários países, sob diferentes clientes e infraestruturas. Cada novo serviço incorporado ao ecossistema precisa ser planejado, justificado, implantado e mantido — o que torna a introdução de qualquer sistema adicional, por mais simples que pareça, uma tarefa nada trivial.

Foi nesse contexto que surgiu uma necessidade aparentemente simples: em determinado ponto do processo, seria necessário utilizar números sequenciais como identificadores para um novo recurso.

== A solução óbvia

A primeira solução considerada foi: _"vamos adotar uma ferramenta pronta que já resolva isso"_. Poderíamos implementar um contador simples usando Redis ou, como alternativa, o próprio Zookeeper.

Ambas as tecnologias oferecem suporte nativo para essa funcionalidade. O Redis dispõe do comando https://redis.io/docs/latest/commands/incr/[INCR], que incrementa um valor associado a uma chave e retorna o resultado atualizado. Já o Zookeeper trabalha com o conceito de nós sequenciais — ou seja, cada novo nó criado recebe automaticamente um sufixo numérico incremental.

[source,ruby]
----
SET mykey "10"
"OK"
INCR mykey
(integer) 11
GET mykey
"11"
----

Uma alternativa mais robusta seria utilizar o https://redis.io/docs/latest/develop/clients/patterns/distributed-locks/[Redis] ou o https://zookeeper.apache.org/doc/r3.8.5/recipes.html#sc_recipes_Locks[Zookeeper] para implementar um Lock Distribuído, garantindo que apenas um processo por vez possa incrementar o contador. No entanto, essa abordagem é mais complexa e desnecessária, uma vez que ambas as tecnologias já oferecem suporte nativo para a funcionalidade de contador.

Em alguns clientes, usar o Zookeeper parecia uma solução simples, já que na época ele ainda era um componente integral do Kafka. No entanto, dois grandes obstáculos se apresentavam.

Primeiro: embora o Kafka utilizasse o Zookeeper para gerenciar o estado do cluster, nossa aplicação não tinha usava essa tecnologia. Além disso, já estava em curso a remoção do Zookeeper do ecossistema do Kafka, justamente por ser considerado uma das fontes de gargalo.

O segundo problema era que muitos clientes já haviam migrado para soluções de Kafka gerenciado (Managed Kafka), onde o serviço é oferecido como um produto em nuvem — e nessas situações, não tínhamos acesso ao Zookeeper. Portanto, mesmo que ele estivesse presente, não podíamos contar com essa solução.

Nessa implementação, a vazão não seria um problema crítico, pois os números sequenciais seriam usados apenas para identificar lotes de registros — e não cada registro individualmente. Isso significa que a taxa de consumo desses sequenciais seria bastante reduzida.

Há ainda outro aspecto fundamental: é impossível estabelecer uma sequência numérica global sem algum tipo de coordenação centralizada. Seja usando Redis ou Zookeeper, a solução continuaria sendo centralizada por natureza — o que, em cenários de alto volume, naturalmente limita a vazão máxima.

Diante dessas restrições, o Apache Kafka surgia como a única solução viável.

== A solução ingênua!

Antes de mergulharmos na solução com Kafka, preciso descartar uma ideia que surgiu durante a discussão — e que, com o perdão da palavra, era tecnicamente ingênua.

_— Por que não usar um simples arquivo de lock compartilhado?_

Sim, essa sugestão realmente apareceu, proposta por um desenvolvedor. Se você também considerou essa possibilidade, vamos revisar alguns conceitos básicos de Sistemas Operacionais e Docker para entender por que ela não se aplica ao nosso cenário.

O bloqueio por arquivo (_file lock_) não é uma funcionalidade do sistema de arquivos em si, mas sim implementada diretamente no kernel do sistema operacional. Isso significa que, para essa solução funcionar, todos os processos envolvidos precisariam estar executando no mesmo sistema operacional — o que equivaleria a ter um "sistema operacional distribuído", um conceito não muito comum na prática.

Além disso, a solução se mostrava inviável em ambientes containerizados. Quando o Docker cria um namespace para um container, ele isola também o sistema de arquivos — o que impede o compartilhamento do _lock_ entre containers.

Para comprovar essa limitação, conduzi um teste simples: o _lock_ funcionou corretamente quando duas JVMs eram executadas diretamente na mesma máquina, sem Docker. No entanto, ao usar containers, o mecanismo deixou de funcionar. Ainda assim, me pediram para testar no Kubernetes... e tive que investir mais tempo para demonstrar, novamente, que a abordagem não era viável.

No final, conseguimos resolver o problema — e comprovei que o Kafka era, de fato, a única solução viável.

== Decompondo o problema

Tínhamos, portanto, um desafio complexo e uma ferramenta que não havia sido projetada para resolvê-lo. Como superar essa situação? A estratégia foi decompor o problema em partes menores.

A geração de números sequenciais pode ser decomposta em dois componentes principais: (i) eleição de um líder e (ii) implementação de uma memória compartilhada. Para simplificar o processo, decidi desenvolver um código básico que serviria como base para a implementação final.

[source,java]
----
try(Leader leader = consensus.acquireLeader()) {
    int currentValue = leader.getInteger(key, 0);
    // use currentValue
    leader.setInteger(key, currentValue + 1);
}
----

Este código foi propositalmente desenvolvido com simplicidade e organizado em três interfaces principais:

* `acquireLeader`: método bloqueante que aguarda até que um líder seja eleito
* `getInteger` e `setInteger`: para leitura e escrita de valores na memória compartilhada

Apenas o líder possui permissão para modificar os valores na memória compartilhada. Por isso, os métodos de leitura e escrita (`getInteger`/`setInteger`) estão disponíveis apenas na interface `Leader`. Já a interface `Consensus` contém apenas o método `acquireLeader`, responsável por coordenar a eleição de forma bloqueante.

Dessa forma, o código principal da aplicação permaneceria isolado da complexidade inerente à eleição de líder e ao gerenciamento da memória compartilhada.

Com a abstração definida, era hora de implementar os algoritmos necessários para colocar toda a solução em funcionamento.

=== Como o Kafka permite consenso

O Apache Kafka é um https://www.researchgate.net/publication/378309830_Middlewares_Orientados_a_Mensagens[middleware orientado a mensagens] que permite a integração entre sistemas por meio do envio e recebimento de eventos. Existem dois modelos de comunicação comuns nesse contexto: Ponto a Ponto e Publish/Subscribe.

O Kafka adota exclusivamente o modelo Publish/Subscribe. Nele, o broker organiza os eventos em tópicos, onde produtores publicam mensagens e consumidores as leem de forma sequencial.

Por sua natureza distribuída, os tópicos no Kafka são divididos em partições. Cada partição atua como um log segmentado — ou seja, o broker sempre grava as mensagens recebidas no final de um arquivo, até que este atinja seu limite e um novo arquivo seja criado.

Ao produzir uma mensagem, o produtor pode escolher explicitamente a partição de destino ou, alternativamente, utilizar uma chave para que a partição seja determinada automaticamente. Em ambos os casos, a mensagem é armazenada de forma sequencial dentro da partição.

.Referências
****
* https://dl.acm.org/doi/10.14778/2824032.2824063[Building a replicated logging system with Apache Kafka]
* https://ieeexplore.ieee.org/document/9005583[Kafka: the Database Inverted, but Not Garbled or Compromised]
* https://notes.stephenholiday.com/Kafka.pdf[Kafka: a Distributed Messaging System for Log Processing]
****

.Anatomia de um Tópico
image::kafka/topico.png[]

É importante destacar que o Kafka não garante a ordem global de consumo das mensagens — apenas a ordem sequencial dentro de uma mesma partição.

Isso significa que, quando um consumidor se inscreve em um tópico, a ordem sequencial só é garantida se o tópico possuir uma única partição. Se houver múltiplas partições, apenas as mensagens com a mesma chave de partição serão consumidas em sequência, pois serão sempre direcionadas à mesma partição.

Dessa forma, o Kafka pode ser utilizado em algoritmos de consenso, uma vez que garante o consumo sequencial de mensagens — desde que elas compartilhem a mesma chave de partição ou o tópico possua apenas uma partição.

Outra característica importante dos tópicos do Kafka são os mecanismos de limpeza, controlados pela configuração `cleanup.policy`. Quando definida como `compact`, essa política impede o crescimento indefinido do tópico, preservando apenas a última mensagem associada a cada chave.

Esse comportamento permite que o tópico funcione como uma espécie de tabela distribuída, que pode ser reconstruída por consumidores a partir do log de eventos. Alguns autores chegam a descrever o Kafka como um "banco de dados invertido" — como se ele utilizasse os mecanismos de sincronização típicos de bancos relacionais, mas aplicados à construção de sistemas distribuídos.

// https://excalidraw.com/#json=jA6Py4ThbyR4b42i_i5vC,X-sQG6LuN85YA3_cEEmh0A 

.Kafka como base de dados invertida
image::kafka/database-inverted.png[]

Você já deve ter percebido onde quero chegar: é possível construir uma tabela distribuída de chave-valor com durabilidade usando o Kafka. Agora, só nos resta resolver a eleição do líder.

### Leader Election (Eleição do Líder)

Para implementar as funcionalidades (i) e (ii), precisamos de um mecanismo de troca de mensagens que seja distribuído e garanta ordem sequencial. Cada participante deve ser capaz de enviar mensagens e consumi-las de forma assíncrona.

Nosso cliente de consenso utilizará dois canais de comunicação: as requisições são enviadas por um produtor e consumidas por um consumidor. No momento do envio, não conhecemos o estado atual da aplicação. Porém, ao consumir as mensagens, temos a garantia de que todos os participantes estão processando o mesmo conjunto de mensagens, na mesma ordem.

Para isso, o algoritmo deve operar através de um tópico de controle com apenas uma partição. Além disso, cada consumidor deve atuar de forma independente — ou seja, sem usar grupos de consumidores —, assegurando que todos recebam e processem integralmente todas as mensagens.

A figura abaixo ilustra esse comportamento. Os sistemas *S1* e *S2* enviam as mensagens *REQ_1* e *REQ_2*, respectivamente, e ambos consomem do tópico de controle as requisições na ordem *REQ_1* → *REQ_2*. Dessa forma, tanto *S1* quanto *S2* terão ciência de que *REQ_1* foi enviada e processada antes de *REQ_2*.

// https://excalidraw.com/#json=amLuEMnf63gLXByN0tN7z,znjVD0cy2eX3wh_-fgyNrQ 
.Kafka sendo usado como tópico de controle
image::kafka/topico-controle.png[]

É importante destacar que timestamps têm utilidade limitada em sistemas distribuídos, já que cada máquina possui seu próprio relógio independente — como já discuti no post https://blog.vepo.dev/posts/relogios-fisicos-e-logicos[Relógios Físicos e Lógicos].

No Kafka, a ordem cronológica dos eventos é determinada pela ordem de recebimento no _broker_, refletida no _offset_ da mensagem. Todos os consumidores recebem as mensagens em ordem crescente de _offset_ e partição, garantindo uma visão consistente e sequencial do fluxo de eventos.

Conhecendo esse comportamento do Kafka, podemos implementar um mecanismo de eleição de líder (_leader election_) baseado no envio e consumo de requisições. Cada processo participante que deseja se tornar líder envia uma requisição de candidatura ao tópico de controle. Da mesma forma, ao finalizar sua atuação como líder, o processo deve enviar uma requisição de renúncia.

// https://editor.plantuml.com/uml/ZLFBJiCm4BpxArRb0WUQUwIW5bGGQYKgn0ziawMrwjYf7wZYTpXmuiWNzCUmawPnGf6uLD7kU6TdPZin5hLrfM0hjOVHoqOJ5b0GJ2l1Spny7ZxK22uMVBsnjy8HgpoSvxn2QMDChtwdf25fdEbLbreLpYhDI1RWsHZREN84etikC50-qyvnm_syRp8ZG0EXmzXmjJ3UbQUr7ZIH_CNlIbx4NFMCERvkqRzRShdjlRZLQaiH6dFBtz3sREYAdRvUwKXvgXs7iYo3H-cP46Q2l3nKGKMIBBmUqaa2MSPOH0JNDnr52kXTqTI0Bw_OgHDWhQTfRHuPsy3weE1Y2ZDJaKQeb8weMK5dLB5sR7ZTg4XXwLQ2cmtJP9sMy3HddUOOLoaXMb3fv-7Q_mompKPJwr3mTvzmVG5zrP4zqo8uuD-UIw2dD38ndX0IQBw3rUFn66PxobrZTgvA1Ol3L-ZwLetbAs-L7aJqTqAQ1A7vHqGHBF9uk7X3x-N6wSI7U3QqGV5uZ56rFSU9l_BmX_y1

.Diagrama de Sequência da Eleição do Líder
image::kafka/leader-election.png[]

Na implementação da eleição de líder, a Fila de Requisições será atualizada exclusivamente pela Thread de Consumo. Essa thread é responsável por consumir mensagens do tópico de controle, adicionar novas requisições à fila e removê-las quando identificar uma mensagem de liberação.

A thread da aplicação deverá enviar uma mensagem de requisição de liderança e aguardar até que essa mensagem se torne a primeira da fila. Quando isso ocorrer, significa que todos os demais processos participantes já enviaram suas requisições e eventuais mensagens de liberação de liderança — garantindo assim que a eleição foi consolidada.

As imagens a seguir ilustram o processo de gerenciamento da fila de requisições.

// https://excalidraw.com/#json=p53_nMkj8YlL6m3sGL7KB,OILOa5-fQvEyBRjJJrGDBA
.Quando os Sistemas A, B e E enviam requisições ao mesmo tempo, elas são ordenadas pelo consumidor e o primeiro a enviar se torna líder. No caso o sistema A.
image::kafka/leader-election-queue-1.png[]

// https://excalidraw.com/#json=LvgbNBuRqt2p4RMf1pTL5,V-gLSUhV3JtK-veU4mQfbg
.Quando o Sistema A envia a mensagem de abdicar a posição de líder, o Sistema B se torna o Líder.
image::kafka/leader-election-queue-2.png[]

// https://excalidraw.com/#json=BVlEQpMe2z64qG-DydwD7,6tPLRsxz0qsq3UqkCmPqdA
.Quando o Sistema B envia a mensagem de abdicar a posição de líder, o Sistema C se torna o Líder.
image::kafka/leader-election-queue-3.png[]

Você pode ter notado que este algoritmo ainda não está completo. Para que funcione corretamente, precisamos tratar duas situações críticas:

1. O que ocorre se um sistema enviar uma requisição e parar de funcionar?
2. Como lidar quando um sistema envia múltiplas requisições?

Para resolver o primeiro caso, enquanto o sistema aguarda para se tornar líder ou durante sua atuação como líder, ele deve enviar mensagens de _heartbeat_ periodicamente. Cada requisição terá uma validade máxima de 2 segundos — caso o sistema não renove o _heartbeat_ dentro desse intervalo, a requisição será removida da fila.

Além disso, quando um sistema detecta que uma requisição expirou, ele deve notificar os demais participantes por meio de uma mensagem de remoção. Isso evita que os outros processos precisem aguardar o tempo total de expiração ao consumirem as requisições, acelerando a resposta do sistema como um todo.

// https://editor.plantuml.com/uml/fL4v3i8m4Ept5QlqHAAbGB4ZKL712vPa1PPosux4YwWeUK4-XkC8J9HqL1xFh6Tsv4GRwemj2sZSgJBLK6KmEYjjk2GK4P96iIvAByfrCVLY2grbonWTL46OCvwujKghuJ6yMFkHKG6WpAYU3BkiBvG1fW5Eym8tUmzttfAr0RcaqYK5VXD7YnqUXsVE7K3kXAw-k-muZd6Bcg-socjqD_YrlR5hUc6Q6GVolzUpzSkwvz53p2i4-jD2XXPZUbPNHbMMfl343OzV-mO0

.Diagrama de Sequência para quando um dos participantes falhar logo após virar líder, sem enviar as mensagens _heartbeat_.0

image::kafka/leader-election-dead.png[]

É crucial que a mensagem de _heartbeat_ seja enviada com uma frequência superior ao tempo de expiração da requisição. Dessa forma, garante-se que o sistema tenha oportunidade de renovar sua presença antes que a requisição seja considerada inválida.

Já o segundo caso pode ser resolvido através de uma modelagem adequada da mensagem de requisição. Cada mensagem deve possuir:

* Uma chave única compartilhada por todos os participantes
* Um campo `id` que identifique o participante
* Um campo `seq` com um número sequencial para controlar a liberação da requisição

Uma requisição será considerada expirada nas seguintes situações:

1. Ao receber um _heartbeat_ com o mesmo `id` e um valor de `seq` superior ao da requisição;
2. Ao receber uma mensagem de `LeaderRelease` com o mesmo `id` e `seq` maior ou igual ao da requisição.

Vale destacar que, logicamente, uma mensagem de `LeaderRelease` só é enviada quando o processo já atua como líder. Isso estabelece uma ordenação lógica entre as mensagens, onde a liberação pressupõe que a liderança foi previamente adquirida.

### Memória Compartilhada

Uma vez eleito líder, o processo pode assumir que consumiu todas as mensagens de líderes anteriores. Dessa forma, o tópico de controle passa a funcionar como uma memória compartilhada, na qual qualquer valor pode ser publicado — utilizando uma chave específica para controlar o acesso a cada elemento compartilhado.

## Kafka como Plataforma de Consenso

Implementar consenso distribuído é desafiador. Esta solução com Kafka é uma opção elegante quando você já está imerso no nosso ecossistema, mas é importante avaliar se a complexidade justifica o benefício para o caso de uso. Para necessidades mais simples, um serviço gerenciado como o Redis ou Zookeeper (quando viável) pode ser mais adequado.

O importante é lembrar que em sistemas distribuídos, nada é simples. A coordenação tem um custo, seja em complexidade, latência ou manutenção. Cabe a nós, engenheiros de software, escolher a moeda de troca mais adequada para cada contexto.


